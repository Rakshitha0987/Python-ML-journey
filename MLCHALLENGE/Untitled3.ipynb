{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635e93b8-93e9-4109-b5c8-6dc13922759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 10:59:35.729608: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-16 10:59:38.972908: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-16 10:59:39.000907: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 10:59:46.521619: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|██████████████████████████████████████| 187/187 [00:00<00:00, 47948.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid label: 10 kilogram to 15 kilogram - Range not supported\n",
      "Error processing image /home/rguktrkvalley/Desktop/images/41wvffSxB4L.jpg: image file is truncated (6 bytes not processed)\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 28s 7s/step - loss: 2.7803 - accuracy: 0.3117 - val_loss: 2.0609 - val_accuracy: 0.4500\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 19s 6s/step - loss: 1.7275 - accuracy: 0.4805 - val_loss: 2.1044 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 1.3058 - accuracy: 0.5844 - val_loss: 2.2312 - val_accuracy: 0.4000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.9976 - accuracy: 0.7403 - val_loss: 2.3712 - val_accuracy: 0.3500\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 17s 5s/step - loss: 0.7664 - accuracy: 0.7922 - val_loss: 2.4760 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.6009 - accuracy: 0.7662 - val_loss: 2.5768 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.4561 - accuracy: 0.8571 - val_loss: 2.6342 - val_accuracy: 0.5500\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.3590 - accuracy: 0.9091 - val_loss: 2.7265 - val_accuracy: 0.4000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.3068 - accuracy: 0.9481 - val_loss: 2.8016 - val_accuracy: 0.4000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 19s 7s/step - loss: 0.2542 - accuracy: 0.9481 - val_loss: 2.8538 - val_accuracy: 0.5500\n",
      "3/3 [==============================] - 16s 5s/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from utils import download_images, parse_string\n",
    "from constants import allowed_units\n",
    "from pathlib import Path\n",
    "\n",
    "# Download images\n",
    "train_df = pd.read_csv('/home/rguktrkvalley/Desktop/train1.csv')\n",
    "test_df = pd.read_csv('/home/rguktrkvalley/Desktop/sample_test.csv')\n",
    "\n",
    "download_folder = '/home/rguktrkvalley/Desktop/images'\n",
    "download_images(train_df['image_link'].tolist() + test_df['image_link'].tolist(), download_folder)\n",
    "\n",
    "# Load model\n",
    "model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model.trainable = False\n",
    "\n",
    "inputs = keras.layers.Input(shape=(224, 224, 3))\n",
    "x = model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(len(allowed_units), activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((224, 224))\n",
    "        image_array = img_to_array(image)\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        image_array = tf.keras.applications.resnet50.preprocess_input(image_array)\n",
    "        return image_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prepare training data\n",
    "train_images = [os.path.join(download_folder, Path(link).name) for link in train_df['image_link']]\n",
    "train_labels = train_df['entity_value'].tolist()\n",
    "\n",
    "# Filter out invalid labels\n",
    "valid_labels = []\n",
    "for label in train_labels:\n",
    "    try:\n",
    "        # Skip ranges or invalid formats\n",
    "        if \"to\" in label:\n",
    "            print(f\"Skipping invalid label: {label} - Range not supported\")\n",
    "            continue\n",
    "        number, unit = parse_string(label)\n",
    "        valid_labels.append(unit)  # Only store the unit\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping invalid label: {label} - {e}\")\n",
    "\n",
    "# Only keep valid images and labels\n",
    "processed_images = []\n",
    "valid_labels_filtered = []\n",
    "\n",
    "for img_path, lbl in zip(train_images, valid_labels):\n",
    "    processed_image = preprocess_image(img_path)\n",
    "    if processed_image is not None:\n",
    "        processed_images.append(processed_image)\n",
    "        valid_labels_filtered.append(lbl)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "train_labels_categorical = tf.keras.utils.to_categorical(\n",
    "    [list(allowed_units).index(lbl) for lbl in valid_labels_filtered if lbl in allowed_units],\n",
    "    num_classes=len(allowed_units)\n",
    ")\n",
    "\n",
    "# Train model\n",
    "if processed_images and len(processed_images) == len(train_labels_categorical):\n",
    "    model.fit(np.concatenate(processed_images),\n",
    "              train_labels_categorical,\n",
    "              epochs=10,\n",
    "              batch_size=32,\n",
    "              validation_split=0.2)\n",
    "else:\n",
    "    print(\"Mismatch in number of processed images and labels. Training aborted.\")\n",
    "\n",
    "# Generate predictions\n",
    "test_images = [os.path.join(download_folder, Path(link).name) for link in test_df['image_link']]\n",
    "test_preds = []\n",
    "\n",
    "for path in test_images:\n",
    "    processed_image = preprocess_image(path)\n",
    "    if processed_image is not None:\n",
    "        test_preds.append(processed_image)\n",
    "\n",
    "# Format output\n",
    "if test_preds:\n",
    "    test_preds_array = model.predict(np.concatenate(test_preds))\n",
    "    output_df = pd.DataFrame({'index': test_df['index']})\n",
    "    output_df['prediction'] = [f\"{1.0} {list(allowed_units)[x]}\" for x in test_preds_array.argmax(axis=1)]\n",
    "    output_df.to_csv('/home/rguktrkvalley/Desktop/pertest_out.csv', index=False)\n",
    "else:\n",
    "    print(\"No valid test images processed. Predictions cannot be generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3481f-f2c4-4cbf-b27c-899d3f76d336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbfd1a-60c9-4417-a784-debf8f5fb775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fba724-1def-452a-9f65-69f632ffa213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044cae71-c96f-43d3-bd0d-e8313e66d547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5b3300-e077-4784-9fbb-2a3b611d5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 11:12:46.520773: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-16 11:12:46.595934: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-16 11:12:46.597269: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 11:12:47.910775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|█████████████████████████████████████| 187/187 [00:00<00:00, 189957.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid label: 10 kilogram to 15 kilogram - Range not supported\n",
      "Error processing image /home/rguktrkvalley/Desktop/images/41wvffSxB4L.jpg: image file is truncated (6 bytes not processed)\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 21s 6s/step - loss: 3.3396 - accuracy: 0.1169 - val_loss: 2.2691 - val_accuracy: 0.3000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 1.8563 - accuracy: 0.3636 - val_loss: 2.1284 - val_accuracy: 0.4000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 1.5429 - accuracy: 0.4286 - val_loss: 2.0017 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 1.0807 - accuracy: 0.6494 - val_loss: 1.9883 - val_accuracy: 0.4000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.8174 - accuracy: 0.7792 - val_loss: 1.9945 - val_accuracy: 0.4500\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.6441 - accuracy: 0.8831 - val_loss: 1.8912 - val_accuracy: 0.4500\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.4870 - accuracy: 0.9091 - val_loss: 1.8022 - val_accuracy: 0.5500\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.3848 - accuracy: 0.8961 - val_loss: 1.8413 - val_accuracy: 0.5500\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.2957 - accuracy: 0.9221 - val_loss: 1.9593 - val_accuracy: 0.4500\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.2397 - accuracy: 0.9221 - val_loss: 2.0479 - val_accuracy: 0.4000\n",
      "3/3 [==============================] - 16s 5s/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from utils import download_images, parse_string\n",
    "from constants import allowed_units\n",
    "from pathlib import Path\n",
    "\n",
    "# Original entity unit map\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', \n",
    "                    'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}\n",
    "\n",
    "# Mapping of abbreviations to full forms\n",
    "unit_abbreviation_map = {\n",
    "    'cm': 'centimetre',\n",
    "    'ft': 'foot',\n",
    "    'in': 'inch',\n",
    "    'm': 'metre',\n",
    "    'mm': 'millimetre',\n",
    "    'yd': 'yard',\n",
    "    'g': 'gram',\n",
    "    'kg': 'kilogram',\n",
    "    'µg': 'microgram',\n",
    "    'mg': 'milligram',\n",
    "    'oz': 'ounce',\n",
    "    'lb': 'pound',\n",
    "    't': 'ton',\n",
    "    'kV': 'kilovolt',\n",
    "    'mV': 'millivolt',\n",
    "    'V': 'volt',\n",
    "    'kW': 'kilowatt',\n",
    "    'W': 'watt',\n",
    "    'cl': 'centilitre',\n",
    "    'cu ft': 'cubic foot',\n",
    "    'cu in': 'cubic inch',\n",
    "    'cup': 'cup',\n",
    "    'dl': 'decilitre',\n",
    "    'fl oz': 'fluid ounce',\n",
    "    'gal': 'gallon',\n",
    "    'imp gal': 'imperial gallon',\n",
    "    'L': 'litre',\n",
    "    'µL': 'microlitre',\n",
    "    'mL': 'millilitre',\n",
    "    'pt': 'pint',\n",
    "    'qt': 'quart'\n",
    "}\n",
    "\n",
    "# Function to map abbreviations to full forms and return the full form if present\n",
    "def get_full_unit(unit):\n",
    "    return unit_abbreviation_map.get(unit, unit)\n",
    "\n",
    "# Function to check if a unit is allowed (including handling abbreviations)\n",
    "def is_unit_allowed(unit):\n",
    "    full_unit = get_full_unit(unit)\n",
    "    return full_unit in allowed_units\n",
    "\n",
    "# Generate the allowed_units set\n",
    "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}\n",
    "\n",
    "# Download images\n",
    "train_df = pd.read_csv('/home/rguktrkvalley/Desktop/train1.csv')\n",
    "test_df = pd.read_csv('/home/rguktrkvalley/Desktop/sample_test.csv')\n",
    "\n",
    "download_folder = '/home/rguktrkvalley/Desktop/images'\n",
    "download_images(train_df['image_link'].tolist() + test_df['image_link'].tolist(), download_folder)\n",
    "\n",
    "# Load the base ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the model architecture\n",
    "inputs = keras.layers.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(len(allowed_units), activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((224, 224))\n",
    "        image_array = img_to_array(image)\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        image_array = tf.keras.applications.resnet50.preprocess_input(image_array)\n",
    "        return image_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prepare training data\n",
    "train_images = [os.path.join(download_folder, Path(link).name) for link in train_df['image_link']]\n",
    "train_labels = train_df['entity_value'].tolist()\n",
    "\n",
    "# Filter out invalid labels\n",
    "valid_labels = []\n",
    "filtered_images = []\n",
    "\n",
    "for img_path, label in zip(train_images, train_labels):\n",
    "    try:\n",
    "        # Skip ranges or invalid formats\n",
    "        if \"to\" in label:\n",
    "            print(f\"Skipping invalid label: {label} - Range not supported\")\n",
    "            continue\n",
    "        number, unit = parse_string(label)\n",
    "        full_unit = get_full_unit(unit)\n",
    "        if full_unit in allowed_units:\n",
    "            valid_labels.append(full_unit)\n",
    "            filtered_images.append(img_path)\n",
    "        else:\n",
    "            print(f\"Skipping label with invalid unit: {label}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping invalid label: {label} - {e}\")\n",
    "\n",
    "# Preprocess the valid images\n",
    "processed_images = []\n",
    "valid_labels_filtered = []\n",
    "\n",
    "for img_path, lbl in zip(filtered_images, valid_labels):\n",
    "    processed_image = preprocess_image(img_path)\n",
    "    if processed_image is not None:\n",
    "        processed_images.append(processed_image)\n",
    "        valid_labels_filtered.append(lbl)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "if valid_labels_filtered:\n",
    "    train_labels_categorical = tf.keras.utils.to_categorical(\n",
    "        [list(allowed_units).index(lbl) for lbl in valid_labels_filtered],\n",
    "        num_classes=len(allowed_units)\n",
    "    )\n",
    "else:\n",
    "    print(\"No valid labels available for training.\")\n",
    "\n",
    "# Train the model\n",
    "if processed_images and len(processed_images) == len(train_labels_categorical):\n",
    "    model.fit(np.concatenate(processed_images),\n",
    "              train_labels_categorical,\n",
    "              epochs=10,\n",
    "              batch_size=32,\n",
    "              validation_split=0.2)\n",
    "else:\n",
    "    print(\"Mismatch in number of processed images and labels. Training aborted.\")\n",
    "\n",
    "# Generate predictions for test images\n",
    "test_images = [os.path.join(download_folder, Path(link).name) for link in test_df['image_link']]\n",
    "test_preds = []\n",
    "\n",
    "for path in test_images:\n",
    "    processed_image = preprocess_image(path)\n",
    "    if processed_image is not None:\n",
    "        test_preds.append(processed_image)\n",
    "\n",
    "# Format and save the prediction output\n",
    "if test_preds:\n",
    "    test_preds_array = model.predict(np.concatenate(test_preds))\n",
    "    output_df = pd.DataFrame({'index': test_df['index']})\n",
    "    output_df['prediction'] = [f\"{1.0} {list(allowed_units)[x]}\" for x in test_preds_array.argmax(axis=1)]\n",
    "    output_df.to_csv('/home/rguktrkvalley/Desktop/pertest_out.csv', index=False)\n",
    "else:\n",
    "    print(\"No valid test images processed. Predictions cannot be generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbefdf5a-9a01-40c5-8931-f3391d554ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287625a-2525-408d-8c3e-c7ac6dcbc70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5286e-5f06-4d14-a2ab-11dbed4e8aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e9c8b-552c-49ae-987a-f496970f671b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7556c83-7126-4e9c-80f4-918bbc3611c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610cf368-fd6a-425e-83b1-7bb69f47cbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 187/187 [00:00<00:00, 26983.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 26s 8s/step - loss: 3.8085 - accuracy: 0.0779 - val_loss: 2.2589 - val_accuracy: 0.4000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 2.0484 - accuracy: 0.3377 - val_loss: 1.8876 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 19s 7s/step - loss: 1.6293 - accuracy: 0.4416 - val_loss: 1.7411 - val_accuracy: 0.5500\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 1.1806 - accuracy: 0.5455 - val_loss: 1.6391 - val_accuracy: 0.7000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.9040 - accuracy: 0.6883 - val_loss: 1.6417 - val_accuracy: 0.6500\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.7308 - accuracy: 0.7143 - val_loss: 1.6154 - val_accuracy: 0.6500\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 21s 7s/step - loss: 0.5440 - accuracy: 0.8442 - val_loss: 1.6582 - val_accuracy: 0.6500\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 20s 7s/step - loss: 0.4376 - accuracy: 0.8442 - val_loss: 1.7849 - val_accuracy: 0.6500\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 20s 6s/step - loss: 0.3621 - accuracy: 0.8831 - val_loss: 1.8921 - val_accuracy: 0.5500\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.2937 - accuracy: 0.9221 - val_loss: 1.9664 - val_accuracy: 0.5500\n",
      "3/3 [==============================] - 17s 5s/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from utils import download_images, parse_string\n",
    "from constants import allowed_units\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Original entity unit map\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', \n",
    "                    'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}\n",
    "\n",
    "# Mapping of abbreviations to full forms\n",
    "unit_abbreviation_map = {\n",
    "    'cm': 'centimetre',\n",
    "    'ft': 'foot',\n",
    "    'in': 'inch',\n",
    "    'm': 'metre',\n",
    "    'mm': 'millimetre',\n",
    "    'yd': 'yard',\n",
    "    'g': 'gram',\n",
    "    'kg': 'kilogram',\n",
    "    'µg': 'microgram',\n",
    "    'mg': 'milligram',\n",
    "    'oz': 'ounce',\n",
    "    'lb': 'pound',\n",
    "    't': 'ton',\n",
    "    'kV': 'kilovolt',\n",
    "    'mV': 'millivolt',\n",
    "    'V': 'volt',\n",
    "    'kW': 'kilowatt',\n",
    "    'W': 'watt',\n",
    "    'cl': 'centilitre',\n",
    "    'cu ft': 'cubic foot',\n",
    "    'cu in': 'cubic inch',\n",
    "    'cup': 'cup',\n",
    "    'dl': 'decilitre',\n",
    "    'fl oz': 'fluid ounce',\n",
    "    'gal': 'gallon',\n",
    "    'imp gal': 'imperial gallon',\n",
    "    'L': 'litre',\n",
    "    'µL': 'microlitre',\n",
    "    'mL': 'millilitre',\n",
    "    'pt': 'pint',\n",
    "    'qt': 'quart'\n",
    "}\n",
    "\n",
    "# Function to map abbreviations to full forms and return the full form if present\n",
    "def get_full_unit(unit):\n",
    "    return unit_abbreviation_map.get(unit, unit)\n",
    "\n",
    "# Function to check if a unit is allowed (including handling abbreviations)\n",
    "def is_unit_allowed(unit):\n",
    "    full_unit = get_full_unit(unit)\n",
    "    return full_unit in allowed_units\n",
    "\n",
    "# Generate the allowed_units set\n",
    "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='image_processing.log', level=logging.INFO)\n",
    "\n",
    "# Preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((224, 224))\n",
    "        image_array = img_to_array(image)\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        image_array = tf.keras.applications.resnet50.preprocess_input(image_array)\n",
    "        return image_array\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download images\n",
    "train_df = pd.read_csv('/home/rguktrkvalley/Desktop/train1.csv')\n",
    "test_df = pd.read_csv('/home/rguktrkvalley/Desktop/sample_test.csv')\n",
    "\n",
    "download_folder = '/home/rguktrkvalley/Desktop/images'\n",
    "download_images(train_df['image_link'].tolist() + test_df['image_link'].tolist(), download_folder)\n",
    "\n",
    "# Load the base ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the model architecture\n",
    "inputs = keras.layers.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(len(allowed_units), activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Prepare training data\n",
    "train_images = [os.path.join(download_folder, Path(link).name) for link in train_df['image_link']]\n",
    "train_labels = train_df['entity_value'].tolist()\n",
    "\n",
    "# Filter out invalid labels\n",
    "valid_labels = []\n",
    "filtered_images = []\n",
    "\n",
    "for img_path, label in zip(train_images, train_labels):\n",
    "    try:\n",
    "        if \"to\" in label:\n",
    "            logging.warning(f\"Skipping range-based label: {label}\")\n",
    "            continue\n",
    "        number, unit = parse_string(label)\n",
    "        full_unit = get_full_unit(unit)\n",
    "        if full_unit in allowed_units:\n",
    "            valid_labels.append(full_unit)\n",
    "            filtered_images.append(img_path)\n",
    "        else:\n",
    "            logging.warning(f\"Invalid unit in label: {label}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error parsing label {label}: {e}\")\n",
    "\n",
    "# Preprocess the valid images\n",
    "processed_images = []\n",
    "valid_labels_filtered = []\n",
    "\n",
    "for img_path, lbl in zip(filtered_images, valid_labels):\n",
    "    processed_image = preprocess_image(img_path)\n",
    "    if processed_image is not None:\n",
    "        processed_images.append(processed_image)\n",
    "        valid_labels_filtered.append(lbl)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "if valid_labels_filtered:\n",
    "    train_labels_categorical = tf.keras.utils.to_categorical(\n",
    "        [list(allowed_units).index(lbl) for lbl in valid_labels_filtered],\n",
    "        num_classes=len(allowed_units)\n",
    "    )\n",
    "else:\n",
    "    print(\"No valid labels available for training.\")\n",
    "\n",
    "# Train the model\n",
    "if processed_images and len(processed_images) == len(train_labels_categorical):\n",
    "    model.fit(np.concatenate(processed_images),\n",
    "              train_labels_categorical,\n",
    "              epochs=10,\n",
    "              batch_size=32,\n",
    "              validation_split=0.2)\n",
    "else:\n",
    "    print(\"Mismatch in number of processed images and labels. Training aborted.\")\n",
    "\n",
    "# Generate predictions for test images\n",
    "test_images = [os.path.join(download_folder, Path(link).name) for link in test_df['image_link']]\n",
    "test_preds = []\n",
    "\n",
    "for path in test_images:\n",
    "    processed_image = preprocess_image(path)\n",
    "    if processed_image is not None:\n",
    "        test_preds.append(processed_image)\n",
    "\n",
    "# Format and save the prediction output\n",
    "if test_preds:\n",
    "    test_preds_array = model.predict(np.concatenate(test_preds))\n",
    "    \n",
    "    # Get the predicted class and its probability\n",
    "    predicted_classes = test_preds_array.argmax(axis=1)\n",
    "    predicted_probabilities = test_preds_array.max(axis=1)\n",
    "    \n",
    "    output_df = pd.DataFrame({'index': test_df['index']})\n",
    "    # Format the output to include the predicted probability and the unit\n",
    "    output_df['prediction'] = [f\"{prob:.2f} {list(allowed_units)[cls]}\" \n",
    "                               for prob, cls in zip(predicted_probabilities, predicted_classes)]\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_df.to_csv('/home/rguktrkvalley/Desktop/pertest_out.csv', index=False)\n",
    "else:\n",
    "    print(\"No valid test images processed. Predictions cannot be generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdb7ad-7c4a-4c0c-8b47-c5cc27ec3482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a00b5-7679-4ded-adc3-68fc7d3f7cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e4ba3-4d7a-4b2d-8b5c-954bddb4f965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a1d8c-2593-46df-a50a-762bd818687b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4652575-716e-4e28-82f8-71e505ac04e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 187/187 [00:00<00:00, 6030.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 32 images and labels.\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 13s 13s/step - loss: 8.0957 - accuracy: 0.0000e+00 - val_loss: 6.8336 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 6.3427 - accuracy: 0.0000e+00 - val_loss: 5.3964 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 4.5982 - accuracy: 0.3600 - val_loss: 3.9776 - val_accuracy: 0.7143\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 2.9230 - accuracy: 0.9200 - val_loss: 2.6644 - val_accuracy: 0.8571\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.5626 - accuracy: 1.0000 - val_loss: 1.6820 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.7412 - accuracy: 1.0000 - val_loss: 1.0780 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.3318 - accuracy: 1.0000 - val_loss: 0.7191 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.1547 - accuracy: 1.0000 - val_loss: 0.5072 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0788 - accuracy: 1.0000 - val_loss: 0.3783 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.2941 - val_accuracy: 1.0000\n",
      "3/3 [==============================] - 17s 5s/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from utils import download_images\n",
    "from pathlib import Path\n",
    "import pytesseract\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='image_processing.log', level=logging.INFO)\n",
    "\n",
    "# OCR-based text extraction using pytesseract\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)  # Extract text using pytesseract OCR\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract unit and value from text\n",
    "def extract_value_and_unit(text):\n",
    "    # Regular expression to find numbers followed by units like grams, kg, etc.\n",
    "    pattern = re.compile(r'(\\d+(\\.\\d+)?\\s?(gram|kilogram|kg|g|pound|lb|ounce|oz))', re.IGNORECASE)\n",
    "    matches = pattern.findall(text)\n",
    "    if matches:\n",
    "        return matches[0][0]  # Return the first match (number + unit)\n",
    "    else:\n",
    "        logging.warning(f\"No value and unit found in text: {text}\")\n",
    "        return None\n",
    "\n",
    "# Preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((224, 224))\n",
    "        image_array = img_to_array(image)\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        image_array = tf.keras.applications.resnet50.preprocess_input(image_array)\n",
    "        return image_array\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the base ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the model architecture\n",
    "inputs = keras.layers.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(1000, activation='softmax')(x)  # Modify based on the actual classes\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Example: Download images and extract labels\n",
    "train_df = pd.read_csv('/home/rguktrkvalley/Desktop/train1.csv')\n",
    "test_df = pd.read_csv('/home/rguktrkvalley/Desktop/sample_test.csv')\n",
    "\n",
    "download_folder = '/home/rguktrkvalley/Desktop/images'\n",
    "download_images(train_df['image_link'].tolist() + test_df['image_link'].tolist(), download_folder)\n",
    "\n",
    "# Prepare training data\n",
    "train_images = [os.path.join(download_folder, Path(link).name) for link in train_df['image_link']]\n",
    "train_labels = train_df['entity_value'].tolist()\n",
    "\n",
    "# Filter out invalid labels and extract text from images\n",
    "valid_labels = []\n",
    "filtered_images = []\n",
    "\n",
    "for img_path in train_images:\n",
    "    # Step 1: Extract text from the image using OCR\n",
    "    extracted_text = extract_text_from_image(img_path)\n",
    "    \n",
    "    if extracted_text:\n",
    "        # Step 2: Extract the value and unit from the text\n",
    "        value_and_unit = extract_value_and_unit(extracted_text)\n",
    "        \n",
    "        if value_and_unit:\n",
    "            # Log the extracted value and unit\n",
    "            logging.info(f\"Extracted from {img_path}: {value_and_unit}\")\n",
    "            valid_labels.append(value_and_unit)\n",
    "            filtered_images.append(img_path)\n",
    "        else:\n",
    "            logging.warning(f\"No valid value and unit found in {img_path}\")\n",
    "    else:\n",
    "        logging.warning(f\"No text extracted from {img_path}\")\n",
    "\n",
    "# Preprocess the valid images\n",
    "processed_images = []\n",
    "valid_labels_filtered = []\n",
    "\n",
    "for img_path, lbl in zip(filtered_images, valid_labels):\n",
    "    processed_image = preprocess_image(img_path)\n",
    "    if processed_image is not None:\n",
    "        processed_images.append(processed_image)\n",
    "        valid_labels_filtered.append(lbl)\n",
    "\n",
    "# Train the model if valid images are available\n",
    "if valid_labels_filtered and processed_images:\n",
    "    print(f\"Training with {len(processed_images)} images and labels.\")\n",
    "    train_labels_categorical = tf.keras.utils.to_categorical(\n",
    "        [0] * len(valid_labels_filtered),  # Dummy labels for now, adjust for your case\n",
    "        num_classes=1000  # Adjust based on your output classes\n",
    "    )\n",
    "    model.fit(np.concatenate(processed_images),\n",
    "              train_labels_categorical,\n",
    "              epochs=10,\n",
    "              batch_size=32,\n",
    "              validation_split=0.2)\n",
    "else:\n",
    "    logging.error(\"No valid labels or images for training.\")\n",
    "\n",
    "# Generate predictions for test images\n",
    "test_images = [os.path.join(download_folder, Path(link).name) for link in test_df['image_link']]\n",
    "test_preds = []\n",
    "\n",
    "for path in test_images:\n",
    "    processed_image = preprocess_image(path)\n",
    "    if processed_image is not None:\n",
    "        test_preds.append(processed_image)\n",
    "\n",
    "# Save predictions\n",
    "if test_preds:\n",
    "    test_preds_array = model.predict(np.concatenate(test_preds))\n",
    "    \n",
    "    # Format and save the prediction output\n",
    "    output_df = pd.DataFrame({'index': test_df['index']})\n",
    "    output_df['prediction'] = ['Predicted_Class' for _ in test_preds_array]  # Placeholder\n",
    "    output_df.to_csv('/home/rguktrkvalley/Desktop/pertest_out.csv', index=False)\n",
    "else:\n",
    "    logging.error(\"No valid test images processed. Predictions cannot be generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454726b-b3c1-4a9c-9c94-bc6cfa70859a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796338e-fffe-401f-9e9a-493258c7ee3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5dc54c-72f5-4a95-8a17-2ff6dbe5a1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c8e489-5210-4014-918d-c9d8e3a74cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 187/187 [00:00<00:00, 30444.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data from /home/rguktrkvalley/Desktop/images/61I9XdN6OFL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71gSRbyXmoL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61BZ4zrjZXL.jpg: ['0.709 gram', '200 milligram', '50 milligram', '25 milligram', '25 milligram', '25 milligram', '25 milligram', '25 milligram', '10 milligram', '051 gram', '02 gram', '0.09 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/612mrlqiI4L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/617Tl40LOXL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61QsBSE7jgL.jpg: ['1400 milligram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81xsq6vf2qL.jpg: ['1400 milligram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71DiLRHeZdL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/91Cma3RzseL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71jBLhmTNlL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81N73b5khVL.jpg: ['4 metre', '5 millimetre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61oMj2iXOuL.jpg: ['35 centimetre', '14 centimetre', '15 centimetre', '16 centimetre', '10 kilogram', '5 kilogram', '25 kilogram', '2545 kilogram', '45 kilogram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/91LPf6OjV9L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81fOxWWWKYL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81dzao1Ob4L.jpg: ['100 gram', '100 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/91-iahVGEDL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81S2+GnYpTL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81e2YtCOKvL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81RNsNEM1EL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/91prZeizZnL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/31EvJszFVfL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61wzlucTREL.jpg: ['22.2 inch']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61sQ+qAKr4L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81x77l2T5NL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71nywfWZUwL.jpg: ['9.10 metre', '2500 metre', '8 volt', '8 volt']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71nywfWZUwL.jpg: ['9.10 metre', '2500 metre', '8 volt', '8 volt']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51WsuKKAVrL.jpg: ['158 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61XGDKap+JL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/715vVcWJxGL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/613v+2W4UwL.jpg: ['55 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71+fn9TWQmL.jpg: ['55 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71aKgRRQ2wL.jpg: ['55 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71rKXZJrh4L.jpg: ['55 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71D824lbRvL.jpg: ['55 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71004c9tzfL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51bQPPtMqYL.jpg: ['4 centimetre', '1.6 inch']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61o2ntPNNgL.jpg: ['800 watt', '36 volt']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61o2ntPNNgL.jpg: ['800 watt', '36 volt']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71IUuTJ8QwL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/915JHkwtcrL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71cjrYndwIL.jpg: ['10 gram', '0.35 ounce']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81hnk2WXO3L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61HXgujoxpL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/613G8GOyLSL.jpg: ['220 millimetre', '8.66 inch', '32 millimetre', '1.25 inch', '305 millimetre', '12.00 inch']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71YyZ2iPyZL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81K3JwUCnQL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41wvffSxB4L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/91cErO-KbLL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/817vo3DcCNL.jpg: ['250 watt']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61AHQ35poHL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61WFh8RCQYL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/711SATIDrmL.jpg: ['169 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61x6RSjwQIL.jpg: ['0.55 inch', '0.55 inch', '0.66 inch', '10 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/613BeFNwHcL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61hWZdkq6WL.jpg: ['750 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71E7CU55dcL.jpg: ['5 centimetre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61c+hSNnnZL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/915w0BdW-gL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61sx0ezNNLL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71ldprwbKrL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71E9iF-bmKL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71sWRp1SNwL.jpg: ['50 gram', '100 gram', '100 gram', '0 gram', '10 milligram', '100 gram', '50 milligram', '59 gram', '0.60 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61Fwq4GeTmL.jpg: ['60 watt']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61-oj+N+BxL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71e6kJLE+LL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71SuzaRS7gL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71nsfFCXF0L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71hgN7yu9OL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61SmT8pkLtL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71ZtDgGX+iL.jpg: ['5 centimetre', '8.1 gram', '13 millimetre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/413FQB0ZMLL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41EjbFu-+yL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71dWDwMhWmS.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61d6Kj80QSL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71bvOuz9w1L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71l0M0tMGjL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71Lpqdrpi4L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71jLIbCcwOL.jpg: ['100 gram', '100 gram', '100 gram', '10 metre', '100 gram', '100 gram', '100 gram', '100 gram', '100 gram', '100 gram', '100 gram', '100 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/718EdwGgyVL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/713twQgCHSL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61I0O1qJbhS.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61eOO5IW4NL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/716AQpAJjZL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71FVeRd2jqL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81njuNSPdjL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51xfRlxWIXL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71duwM3SjpL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/612xIhPMHqL.jpg: ['160 millimetre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51b9JEHOriL.jpg: ['6.5 gram', '19 centimetre', '5 centimetre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81lgxfKqUUL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/814sAvV89SL.jpg: ['42 gram', '0 gram', '0 gram', '39 gram', '28 gram', '28 gram', '0 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61cMeogK8gL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/811VfR10yxL.jpg: ['5.5 volt', '0.55 watt', '7 metre', '2.5 metre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71WLYfmMqQL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61Dq3LRei9L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71XK5d3Oh9L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61kyBEJYDeL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71uQmsTESvL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71jG8BOi4WL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41-NCxNuBxL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41-NCxNuBxL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/417NJrPEk+L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/417SThj+SrL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/417SThj+SrL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41ADVPQgZOL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41nblnEkJ3L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41nblnEkJ3L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41o3iis9E7L.jpg: ['1.2 kilogram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41pvwR9GbaL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41uwo4PVnuL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41uwo4PVnuL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41ygXRvf8lL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41ygXRvf8lL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/41zgjN+zW3L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51+oHGvSvuL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51+oHGvSvuL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51-WIOx5pxL.jpg: ['6.2 inch', '16 centimetre', '9.4 inch', '24 centimetre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51-WIOx5pxL.jpg: ['6.2 inch', '16 centimetre', '9.4 inch', '24 centimetre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/510xYFNYQ8L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/510xYFNYQ8L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/510xYFNYQ8L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/514bY8c4ZIL.jpg: ['69 centimetre', '27.2 inch']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/514bY8c4ZIL.jpg: ['69 centimetre', '27.2 inch']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/514pScQdlCL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/514pScQdlCL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51BEuVR4ZzL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51BEuVR4ZzL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51EBBqNOJ1L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51EBBqNOJ1L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51EBBqNOJ1L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51FSlaVlejL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51H+mX2Wk7L.jpg: ['9 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51H+mX2Wk7L.jpg: ['9 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51KykmLgc0L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51P0IuT6RsL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51Su6zXkAsL.jpg: ['913.8 centimetre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51bEy0J5wLL.jpg: ['30.5 centimetre', '24.1 centimetre', '10.7 centimetre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51cPZYLk2YL.jpg: ['16 centimetre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51fAzxNm+cL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51fAzxNm+cL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51fAzxNm+cL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51jTe522S2L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51kdBAv6ImL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51kdBAv6ImL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51l6c6UcRZL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51oaOP8qJlL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51oaOP8qJlL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51r7U52rh7L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51r7U52rh7L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51r7U52rh7L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51tEop-EBJL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51vwYpDz2tL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51y79cwGJFL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51y79cwGJFL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/51y79cwGJFL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/613P5cxQH4L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/613P5cxQH4L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/614hn5uX9MS.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/615Cjzm6pyL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/615Cjzm6pyL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61C+fwVD6dL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61E2XRNSdYL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61G8bvWOb-L.jpg: ['3 millimetre', '27.8 millimetre', '42.0 millimetre', '5 gram', '5 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61G8bvWOb-L.jpg: ['3 millimetre', '27.8 millimetre', '42.0 millimetre', '5 gram', '5 gram']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61O+Yi09tyL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/61lX6IP1SVL.jpg: ['17.6 volt', '17.6 volt']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71Qk6hR9-WL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71Qk6hR9-WL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71UN1IxKp4L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71UN1IxKp4L.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71UYDq4nfnL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71UYDq4nfnL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71WAjPMQDWL.jpg: ['14.8 inch', '110 watt', '3.2 litre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71WAjPMQDWL.jpg: ['14.8 inch', '110 watt', '3.2 litre']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71afEPoRGsL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71afEPoRGsL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71eCfiIG-AL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71fWddA0+yL.jpg: ['60 watt']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71ta6wY3HtL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71ta6wY3HtL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71v+pim0lfL.jpg: ['0 metre', '4.1 inch', '3.8 centimetre', '1.5 inch', '16.9 inch', '43 centimetre', '7 inch', '5.3 inch']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/71v+pim0lfL.jpg: ['0 metre', '4.1 inch', '3.8 centimetre', '1.5 inch', '16.9 inch', '43 centimetre', '7 inch', '5.3 inch']\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81IYdOV0mVL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81PG3ea0MOL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81aZ2ozp1GL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81qUmRUUTTL.jpg: []\n",
      "Extracted data from /home/rguktrkvalley/Desktop/images/81qUmRUUTTL.jpg: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from utils import download_images\n",
    "from constants import allowed_units\n",
    "from pathlib import Path\n",
    "import pytesseract\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Original entity unit map\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', \n",
    "                    'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}\n",
    "\n",
    "# Mapping of abbreviations to full forms\n",
    "unit_abbreviation_map = {\n",
    "    'cm': 'centimetre',\n",
    "    'ft': 'foot',\n",
    "    'in': 'inch',\n",
    "    'm': 'metre',\n",
    "    'mm': 'millimetre',\n",
    "    'yd': 'yard',\n",
    "    'g': 'gram',\n",
    "    'kg': 'kilogram',\n",
    "    'µg': 'microgram',\n",
    "    'mg': 'milligram',\n",
    "    'oz': 'ounce',\n",
    "    'lb': 'pound',\n",
    "    't': 'ton',\n",
    "    'kV': 'kilovolt',\n",
    "    'mV': 'millivolt',\n",
    "    'V': 'volt',\n",
    "    'kW': 'kilowatt',\n",
    "    'W': 'watt',\n",
    "    'cl': 'centilitre',\n",
    "    'cu ft': 'cubic foot',\n",
    "    'cu in': 'cubic inch',\n",
    "    'cup': 'cup',\n",
    "    'dl': 'decilitre',\n",
    "    'fl oz': 'fluid ounce',\n",
    "    'gal': 'gallon',\n",
    "    'imp gal': 'imperial gallon',\n",
    "    'L': 'litre',\n",
    "    'µL': 'microlitre',\n",
    "    'mL': 'millilitre',\n",
    "    'pt': 'pint',\n",
    "    'qt': 'quart'\n",
    "}\n",
    "\n",
    "# Function to map abbreviations to full forms and return the full form if present\n",
    "def get_full_unit(unit):\n",
    "    return unit_abbreviation_map.get(unit, unit)\n",
    "\n",
    "# Generate the allowed_units set\n",
    "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='image_processing.log', level=logging.INFO)\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        # Use pytesseract to extract text\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text if text else \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def parse_extracted_text(text):\n",
    "    # Ensure text is not None or empty\n",
    "    if not text or not isinstance(text, str):\n",
    "        logging.warning(\"No valid text extracted to parse.\")\n",
    "        return []\n",
    "\n",
    "    # Regular expression to find patterns like \"100 g\" or \"120 kg\"\n",
    "    pattern = r\"(\\d+\\.?\\d*)\\s*(\\w+)\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    # Process and filter matches to return valid ones\n",
    "    valid_matches = []\n",
    "    for number, unit in matches:\n",
    "        full_unit = get_full_unit(unit)\n",
    "        if full_unit in allowed_units:\n",
    "            valid_matches.append(f\"{number} {full_unit}\")\n",
    "    \n",
    "    return valid_matches\n",
    "\n",
    "# Preprocess images for the neural network model\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((224, 224))\n",
    "        image_array = img_to_array(image)\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        image_array = tf.keras.applications.resnet50.preprocess_input(image_array)\n",
    "        return image_array\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download images\n",
    "train_df = pd.read_csv('/home/rguktrkvalley/Desktop/train1.csv')\n",
    "test_df = pd.read_csv('/home/rguktrkvalley/Desktop/sample_test.csv')\n",
    "\n",
    "download_folder = '/home/rguktrkvalley/Desktop/images'\n",
    "download_images(train_df['image_link'].tolist() + test_df['image_link'].tolist(), download_folder)\n",
    "\n",
    "# Extract text and parse units for train images\n",
    "for img_path in [os.path.join(download_folder, Path(link).name) for link in train_df['image_link']]:\n",
    "    text = extract_text_from_image(img_path)\n",
    "    parsed_data = parse_extracted_text(text)\n",
    "    print(f\"Extracted data from {img_path}: {parsed_data}\")\n",
    "\n",
    "# Process test images similarly and create predictions\n",
    "for img_path in [os.path.join(download_folder, Path(link).name) for link in test_df['image_link']]:\n",
    "    text = extract_text_from_image(img_path)\n",
    "    parsed_data = parse_extracted_text(text)\n",
    "    print(f\"Extracted data from {img_path}: {parsed_data}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
